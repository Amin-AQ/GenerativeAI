{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA1.1 - Word Embeddings\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will learn how to train your own word embeddings using two approaches, then explore some of the fun things you can do with them.\n",
    "\n",
    "Word Embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "For reference and additional details, please go throught the following resources:\n",
    "\n",
    "1) Chapter 6 of [the SLP3 book](https://web.stanford.edu/~jurafsky/slp3)\n",
    "2) This [nice writeup by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/).\n",
    "3) [LSTMs Basics](https://medium.com/linagoralabs/next-word-prediction-a-complete-guide-d2e69a7a09e6)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Implementing `word2vec` [80 points]\n",
    "\n",
    "In this part, you will implement the `word2vec` algorithm. \n",
    "\n",
    "While `word2vec` is more of a _framework_ for learning word embeddings, we will focus on the `SkipGram` model, specifically how it was trained in the original 2013 paper. Your primary references for understanding this algorithm will be the lecture slides, and the two aforementioned links.\n",
    "\n",
    "You will be working with the *greatest* work of literature ever written: ~~Shrek 2~~ **The Lord of the Rings** - specifically the first book, *The Fellowship of the Ring*. \n",
    "\n",
    "While this book is a masterpiece, it can take a while to train embeddings on the entire text. So, we will be working with a subset of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aminq\\documents\\github\\generativeai\\1.1\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aminq\\documents\\github\\generativeai\\1.1\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aminq\\documents\\github\\generativeai\\1.1\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aminq\\documents\\github\\generativeai\\1.1\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.0 MB 1.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.8/8.0 MB 1.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.8/8.0 MB 1.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.6/8.0 MB 1.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.8/8.0 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.1/8.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.4/8.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.6/8.0 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.1/8.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.4/8.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.7/8.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.9/8.0 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.2/8.0 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.7/8.0 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.0 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.2/8.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/8.0 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.3/8.0 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.3/8.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.0 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 1.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/12.9 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.0/12.9 MB 1.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 1.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 1.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.1/12.9 MB 1.4 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.4/12.9 MB 1.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.6/12.9 MB 1.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.9/12.9 MB 1.4 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.1/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.9/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 4.2/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 4.5/12.9 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.7/12.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.0/12.9 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.3/12.9 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.9 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 7.1/12.9 MB 1.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.3/12.9 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.9/12.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.1/12.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.4/12.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.4/12.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.4/12.9 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.7/12.9 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.7/12.9 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.9/12.9 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.2/12.9 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.9 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 10.0/12.9 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.2/12.9 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.5/12.9 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.9 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.9 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.9 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.8/12.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.6/12.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.6 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.6 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.0/2.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.3/2.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, click, nltk, contourpy, matplotlib\n",
      "Successfully installed click-8.1.8 contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 nltk-3.9.1 numpy-2.2.3 pillow-11.1.0 pyparsing-3.2.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T18:57:37.898239Z",
     "iopub.status.busy": "2025-01-18T18:57:37.897734Z",
     "iopub.status.idle": "2025-01-18T18:57:41.282650Z",
     "shell.execute_reply": "2025-01-18T18:57:41.281515Z",
     "shell.execute_reply.started": "2025-01-18T18:57:37.898196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import in the libraries\n",
    "# Note: you are NOT allowed to use any other libraries or functions outside of these\n",
    "# Use the following seeds for reproducibility\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "#nltk.download('punkt_tab')\n",
    "np.random.seed(22)\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our raw data\n",
    "\n",
    "In the cell below, you will read in the data as one very long string.\n",
    "\n",
    "This will be followed by creating a `Dataset` class that will be helpful in working with your dataset when training the model. The `Dataset` class should have the following attributes/methods:\n",
    "\n",
    "- `__init__(self, data)` - the constructor that takes in the data and initializes the necessary attributes.\n",
    "\n",
    "- `data` - the data that is passed in. You can apply a very simple preprocessing pipeline: (1) substitute `,!?;-` (i.e. these five punctuation marks) with a period (i.e. `.`), (2) lowercase all the text, and (3) extract only those characters that are alphabetic or a period.\n",
    "\n",
    "- `tokens` - a list of all the tokens in the data. It might be helpful to use the `nltk.word_tokenize` function already imported for you.\n",
    "\n",
    "- `vocab` - a set of all the unique tokens in the data. Be sure to sort this and convert it to a list as to have a consistent ordering.\n",
    "\n",
    "- `vocab_size` - the length of the vocabulary.\n",
    "\n",
    "- `stoi` - a mapping from the word (s) to their index (i) in the vocab. It is important to have sorted your vocab before creating this mapping.\n",
    "\n",
    "- `itos` - a mapping from the index (i) to the word (s) in the vocab.\n",
    "\n",
    "The two mappings will be helpful in fetching your Embeddings later on, since your Embeddings will be a matrix of shape `(vocab_size, embedding_dim)` and the ordering will be dependent on your vocabulary's ordering.\n",
    "\n",
    "You might find the following boilerplate helpful:\n",
    "```python\n",
    "# Substitute the specified punctuation with periods\n",
    "data = ...\n",
    "\n",
    "# Tokenize the above plaintext to get a list of tokens\n",
    "tokens = ...\n",
    "\n",
    "# Only grab those tokens that are alphabetic or a period (can use .isalpha() here, and a == check for '.') - a list comprehension might be helpful\n",
    "tokens = ...\n",
    "\n",
    "# Lowercase all the tokens\n",
    "tokens = ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J. R. R. Tolkien Â— The Lord Of The Rings. (1/4)\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "     THE LORD OF THE RINGS\n",
      "\n",
      "              by\n",
      "\n",
      "     J. R. R. TOLKIEN\n",
      "\n",
      "\n",
      "\n",
      " Part 1: The Fellowship of the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1025058"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "with open(\"./The Fellowship of the Ring.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[:200]) # print out the first 200 chars\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dataset: 209939\n",
      "Vocabulary size: 8413\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, data: str):\n",
    "        self.data = data\n",
    "        self.data = re.sub('[,!?;-]','.',self.data)\n",
    "        self.tokens = word_tokenize(self.data)\n",
    "        self.tokens = [token.lower() for token in self.tokens if token.isalpha() or token == '.']\n",
    "        self.vocab = sorted(list(set(self.tokens)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.stoi = {word:index for index, word in enumerate(self.vocab)}\n",
    "        self.itos = {index:word for index, word in enumerate(self.vocab)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "dataset = Dataset(data)\n",
    "print(f\"Number of tokens in dataset: {len(dataset)}\")\n",
    "print(f\"Vocabulary size: {dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset\n",
    "\n",
    "Now for the fun part of the dataset preparation: creating the windows!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://jalammar.github.io/images/word2vec/skipgram-sliding-window-5.png\">\n",
    "</center>\n",
    "\n",
    "Recall in class you learned about sliding a window over the text to create the `(context, target)` pairs. You will implement this in the function below. \n",
    "\n",
    "We will adopt the following convention: the target word is at the center of a window, and the context words are the words surrounding the target word, with `ctx_size` tokens on either side.\n",
    "\n",
    "You will implement this to work with a list of tokens (whether that be in string-form, or as indices). The function should return a list of tuples, where each tuple is a pair of the form `(context, target)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The `range` function will return you an iterator where you can specify the start and final indices, as well as the jumps in between. Use this in constructing your for loop.</li>\n",
    "    <li>One easy way to do this is to begin the loop at the position corresponding to the first center word, then to grab the words to the left and the right in one list. After doing this, your loop moves to the next iter. Be careful with the starting and end iteration variables for your loop.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'you', '.'] ---> are\n",
      "['how', 'are', '.', 'i'] ---> you\n",
      "['are', 'you', 'i', 'am'] ---> .\n",
      "['you', '.', 'am', 'under'] ---> i\n",
      "['.', 'i', 'under', 'the'] ---> am\n",
      "['i', 'am', 'the', 'water'] ---> under\n",
      "['am', 'under', 'water', '.'] ---> the\n",
      "['under', 'the', '.', 'please'] ---> water\n",
      "['the', 'water', 'please', 'help'] ---> .\n",
      "['water', '.', 'help', 'me'] ---> please\n",
      "['.', 'please', 'me', '.'] ---> help\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def get_windows(\n",
    "        data: List[Union[str, int]], \n",
    "        ctx_size: int\n",
    "    ):\n",
    "    '''\n",
    "    Generates the windows to be used later for dataset creation\n",
    "\n",
    "    Takes in a list of tokens (as strings or integers/indices) and a context size.\n",
    "    This slides a window of size 2*ctx_size + 1 over the data, producing\n",
    "    a list of context words and corresponding target words.\n",
    "\n",
    "    Note that ctx_size is the number of context words on EITHER side of the center word.\n",
    "    '''\n",
    "    outside_words = []\n",
    "    center_words = []\n",
    "\n",
    "    # Iterate over the data with a sliding window\n",
    "    for i in range(ctx_size, len(data) - ctx_size):\n",
    "        # The center word is the current word\n",
    "        center_words.append(data[i])\n",
    "        # Context words are the words around the center word\n",
    "        context = data[i - ctx_size:i] + data[i + 1:i + ctx_size + 1]\n",
    "        outside_words.append(context)\n",
    "    \n",
    "    return outside_words, center_words\n",
    "\n",
    "# Test the function\n",
    "owords, cwords = get_windows(\n",
    "    word_tokenize(\"hello how are you. i am under the water. please help me.\"),\n",
    "    2\n",
    ")\n",
    "\n",
    "for i in range(len(owords)):\n",
    "    print(f\"{owords[i]} ---> {cwords[i]}\")\n",
    "\n",
    "# Verify the test case\n",
    "assert owords[1] == ['how', 'are', '.', 'i'] and cwords[1] == 'you' and len(owords) == 11, \\\n",
    "    \"Test failed\"\n",
    "\n",
    "print(\"Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to creating our model, recall that a key component of the algorithm was **Negative Sampling** so that our model was able to see occurences of words that _didn't_ appear in the context. We will implement this in the next part.\n",
    "\n",
    "Your `sample_neg_word` is a utility function that will continue sampling a word from the specific vocabulary until it is not the word you have specified. \n",
    "\n",
    "When actually implementing the negative sampling, you will sample tokens that are _not_ the center word only - many implementations ignore the exclusion of the context words, so we will do this same simplification here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use the `np.random.choice` function to sample an element from a list.</li>\n",
    "    <li>You can think about running a loop where you keep sampling until you have something that isn't the word to exclude. This is the only check you must make.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T18:57:43.843430Z",
     "iopub.status.busy": "2025-01-18T18:57:43.842941Z",
     "iopub.status.idle": "2025-01-18T18:57:43.866498Z",
     "shell.execute_reply": "2025-01-18T18:57:43.865387Z",
     "shell.execute_reply.started": "2025-01-18T18:57:43.843334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To create the dataset, we need to get positive and negative samples according to the windows made\n",
    "\n",
    "def sample_neg_word(to_exclude: str,\n",
    "                    vocab: List[str]):\n",
    "        '''\n",
    "        Samples a negative word from the vocab, excluding the word to_exclude\n",
    "        '''\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "        while True:\n",
    "            sample_word = np.random.choice(vocab)\n",
    "\n",
    "            if sample_word != to_exclude:\n",
    "\n",
    "                return sample_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the actual model\n",
    "\n",
    "Now you will make use of the `Dataset` class and the `sample_neg_word` function to implement the `SkipGram` model.\n",
    "\n",
    "Recall the steps of the model:\n",
    "\n",
    "1. Randomly initialize two matrices: `W` and `C` of shape `(vocab_size, embedding_dim)`. These will be your center/target word and context embeddings respectively.\n",
    "\n",
    "2. Being looping through each `(context, target)` pair in your dataset.\n",
    "\n",
    "    2.1. For each pair, sample `K` negative words from the vocabulary.\n",
    "\n",
    "    2.2. Compute the loss for the context and target word, as well as the negative samples.\n",
    "\n",
    "    2.3. Compute the gradients for the context and target word, as well as the `K` negative samples.\n",
    "\n",
    "    2.4. For each of these computed gradients, update the corresponding embeddings.\n",
    "\n",
    "3. Repeat this process for `num_epochs`.\n",
    "\n",
    "\n",
    "Recall the formulas for the Loss function and the gradients:\n",
    "\n",
    "$$L_{CE} = -\\log \\sigma(c_{pos} \\cdot w) - \\sum_{i=1}^{K} \\log \\sigma(-c_{neg_i} \\cdot w)$$\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial w} = [\\sigma(c_{pos} \\cdot w) - 1]c_{pos} + \\sum_{i=1}^{K} [\\sigma(c_{neg_i} \\cdot w)]c_{neg_i}$$\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial c_{pos}} = [\\sigma(c_{pos} \\cdot w) - 1]w$$\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial c_{neg_i}} = \\sigma(c_{neg_i} \\cdot w)w$$\n",
    "\n",
    "Where $c_{pos}$ is the context word, $w$ is the target word, and $c_{neg_i}$ is the $i^{th}$ negative sample.\n",
    "\n",
    "#### Implementation notes\n",
    "\n",
    "- You will be implementing all of this in the `word2vec` class, inside the `fit` function. Your embedding matrices will be initialized in the `__init__` function, according to the arguments passed in.\n",
    "\n",
    "- You **must** print out your loss after every epoch (i.e. after every pass through the dataset). If you have implemented everything perfectly, you should see the loss decreasing over time, with no numerical overflows/underflows or the like.\n",
    "\n",
    "- Your `fit` function will return the list of losses over each epoch - this will be used to visualize the training process.\n",
    "\n",
    "- <font size=\"3\" color=\"red\"><b>Do not tamper with code you haven't been permitted to modify.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T18:57:43.867833Z",
     "iopub.status.busy": "2025-01-18T18:57:43.867511Z",
     "iopub.status.idle": "2025-01-18T18:57:43.891895Z",
     "shell.execute_reply": "2025-01-18T18:57:43.890828Z",
     "shell.execute_reply.started": "2025-01-18T18:57:43.867805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the data\n",
    "# The dataset is very large, so we will only use the first 2000 tokens for now\n",
    "# There will be 3 words on each side of the center word\n",
    "context_words, target_words = get_windows(\n",
    "    dataset.tokens[:2000], \n",
    "    ctx_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>A simple (but slightly inefficient) way to implement this would be to use two `for` loops (ignoring the one for sampling the negative words): one for the center word/window, an inner one for each context word so you have a (ctx, target) pair. This is done since you are passing the windows as they are into the function.</li>\n",
    "    <li>Be VERY careful about which variable is a string, and which is the corresponding index in the vocab. You can use the `encode` function to move from string to index.</li>\n",
    "    <li>Print out the shapes or use `assert` statements to ensure the shapes are the way you'd expect them. Broadcasting can mess things up if you're not careful.</li>\n",
    "    <li>Never forget: the gradients that are being calculated will ONLY update the embeddings for the corresponding tokens in the matrices, NOT THE ENTIRE MATRIX.</li>\n",
    "    <li>An easy way to do this is to make a matrix of zeros representing the gradients for one of the Embedding matrices. When you create the gradient vector(s) for the target/context/negative samples, you can simply replace the corresponding row in that zeros matrix with that vector. The update equation will be very easy to implement.</li>\n",
    "    <li>If you're running into overflows/underflows or numerical instability, there is something wrong with your implementation. If you're sure everything is correct, double-check your equations with the lecture slides - one wrong sign for a gradient cost me an hour of debugging ;-;</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1,  -2,  -3,  -4],\n",
       "       [ -9, -10, -11, -12]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-v[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T18:57:43.893173Z",
     "iopub.status.busy": "2025-01-18T18:57:43.892891Z",
     "iopub.status.idle": "2025-01-18T18:57:43.965097Z",
     "shell.execute_reply": "2025-01-18T18:57:43.963906Z",
     "shell.execute_reply.started": "2025-01-18T18:57:43.893148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x : np.ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class word2vec:\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size : int, \n",
    "                 stoi : dict, \n",
    "                 emb_dim : int):\n",
    "        self.stoi = stoi\n",
    "        self.W = np.random.random((vocab_size, emb_dim))\n",
    "        self.C = np.random.random((vocab_size, emb_dim))\n",
    "\n",
    "    def __call__(self, x : int):\n",
    "        return (self.W[x] + self.C[x]) / 2\n",
    "    \n",
    "    \n",
    "    def encode(self, x : Union[str, List[str]]):\n",
    "        return [dataset.stoi[i] for i in x] if isinstance(x, list) else self.stoi[x]\n",
    "\n",
    "    def fit(self, \n",
    "            context_words : List[List[str]], \n",
    "            target_words : List[str], \n",
    "            num_epochs : int = 5, \n",
    "            learning_rate : float = 0.001, \n",
    "            K : int = 5):\n",
    "        '''\n",
    "        Runs the training algorithm for the word2vec model\n",
    "        '''\n",
    " \n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            for context, target in zip(context_words, target_words):\n",
    "                for input_word in context:\n",
    "                    negative_sampals = [sample_neg_word(input_word,dataset.vocab) for _ in range(K)]\n",
    "                    negative_sampals_indices = self.encode(negative_sampals)\n",
    "                    input_word_index = self.encode(input_word)\n",
    "                    target_index = self.encode(target)\n",
    "\n",
    "                    c_pos = self.W[input_word_index]\n",
    "                    c_neg = self.C[negative_sampals_indices]\n",
    "                    w = self.C(target_index)\n",
    "                    print(f'C_pos: {c_pos.shape}, C_neg: {c_neg.shape}, w: {w.shape}')\n",
    "                    c_pos_w_dot = np.dot(c_pos,w)\n",
    "                    c_neg_w_dot = np.dot(c_neg,w)\n",
    "                    L_CE = -np.log(sigmoid(c_pos_w_dot)) - np.sum(np.log(sigmoid(-c_neg_w_dot)))\n",
    "                    epoch_loss+=L_CE\n",
    "                    d_lce_w = (sigmoid(c_pos_w_dot)-1) * c_pos + np.sum(sigmoid(c_neg_w_dot)*c_neg)\n",
    "                    d_lce_cpos = (sigmoid(c_pos_w_dot)-1) * w\n",
    "                    d_lce_cneg = sigmoid(c_neg_w_dot) * w\n",
    "\n",
    "                    \n",
    "\n",
    "            epoch_loss = epoch_loss / len(target_words)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.3f}\")\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "        return losses\n",
    "\n",
    "w2v_model = word2vec(vocab_size=dataset.vocab_size,\n",
    "                     stoi=dataset.stoi,\n",
    "                     emb_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['tolkien', 'the', 'lord', 'the', 'rings', '.'], 'of'),\n",
       " (['the', 'lord', 'of', 'rings', '.', 'the'], 'the'),\n",
       " (['lord', 'of', 'the', '.', 'the', 'lord'], 'rings'),\n",
       " (['of', 'the', 'rings', 'the', 'lord', 'of'], '.'),\n",
       " (['the', 'rings', '.', 'lord', 'of', 'the'], 'the')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ctx, trgt) for ctx, trgt in zip(context_words, target_words)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "losses = w2v_model.fit(context_words,\n",
    "                       target_words,\n",
    "                       num_epochs=10)\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curve for word2vec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does contrastive learning improve the quality of word embeddings by distinguishing between similar and dissimilar pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why is contrastive loss effective in learning meaningful word embeddings, and how does it guide the model to improve embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Hopefully you were able to implement and train the model successfully!\n",
    "\n",
    "Now to actually examine the embeddings you've trained. We will do one small exercise involving checking the similarity of pairs of words in the vocabulary.\n",
    "\n",
    "This means you will need to implement the Cosine Similarity function. Recall that the Cosine Similarity between two vectors $u$ and $v$ is given by:\n",
    "$$ \\text{Cosine Similarity}(u, v) = \\frac{u \\cdot v}{||u|| \\cdot ||v||} $$\n",
    "\n",
    "You will implement this in the `cosine_similarity` function below. You will then use this function on a series of predefined word pairs to see how similar they are in the embedding space: this entails that they showed up in similar contexts in the text.\n",
    "\n",
    "Since most people are not cultured enough to have read the Lord of the Rings, here are some notes to make sense of what you _should_ see:\n",
    "\n",
    "- `Frodo` and `Sam` are best friends, so they should have a high similarity.\n",
    "\n",
    "- `Gandalf` has been referred to as Gandalf the `White`.\n",
    "\n",
    "- `Frodo` must make an arduous journey to `Mordor`.\n",
    "\n",
    "- `Aragorn` is close to Frodo, but hasn't been to his home of the `Shire`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    '''\n",
    "    Implements the cosine similarity for vectors u and v\n",
    "    '''\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "    return sim\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"frodo\"]],\n",
    "    w2v_model.C[dataset.stoi[\"sam\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"gandalf\"]],\n",
    "    w2v_model.C[dataset.stoi[\"white\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"mordor\"]],\n",
    "    w2v_model.C[dataset.stoi[\"frodo\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"shire\"]],\n",
    "    w2v_model.C[dataset.stoi[\"aragorn\"]]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform word vector arithmetic to solve the analogy \"king - man + woman ≈ queen.\" By calculating the cosine similarity between the resulting vector and all other word embeddings, we identify the most similar words to the computed result. This helps evaluate how well the embeddings capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_analogous_word(word1, word2, word3, model, top_n=5):\n",
    "    \"\"\" Returns the top N words that solve the analogy: word1 - word2 + word3 \"\"\"\n",
    "    \n",
    "    idx1, idx2, idx3 = model.stoi[word1], model.stoi[word2], model.stoi[word3]\n",
    "    \n",
    "    embeddings = (model.W + model.C) / 2         # Use the average of W and C embeddings\n",
    "    \n",
    "    vec1 = embeddings[idx1]                      # Get the embeddings for the words\n",
    "    vec2 = embeddings[idx2]\n",
    "    vec3 = embeddings[idx3]\n",
    "    \n",
    "    result_vector = vec1 - vec2 + vec3           # Perform the vector arithmetic\n",
    "    \n",
    "    similarities = np.dot(embeddings, result_vector) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(result_vector))\n",
    "    most_similar_indices = np.argsort(similarities)[::-1][:top_n]      # Get the indices of the most similar word\n",
    "    return [list(model.stoi.keys())[i] for i in most_similar_indices if list(model.stoi.keys())[i] not in [word1, word2, word3]]    # Return the top N most similar words\n",
    "\n",
    "analogy_results = get_analogous_word('king', 'man', 'woman', w2v_model, top_n=5)\n",
    "print(f'Analogy result: king - man + woman ≈ {analogy_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of Word Embeddings for Analogy Task Using PCA**\n",
    "\n",
    "In this section, we will visualize the word embeddings for the words \"king,\" \"man,\" \"woman,\" and \"queen\" by projecting them into a 2D space using PCA. This will help us assess whether the learned embeddings capture the semantic relationships between these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "words = ['king', 'man', 'woman', 'queen']\n",
    "\n",
    "embeddings_for_words = [w2v_model.W[w2v_model.stoi[word]] for word in words]\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_for_words)\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
    "    plt.text(reduced_embeddings[i, 0] + 0.05, reduced_embeddings[i, 1] + 0.05, word, fontsize=12)\n",
    "\n",
    "plt.title('2D PCA of Word Embeddings for Analogy Task: king - man + woman ≈ queen')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Comment on how the quality of the embeddings impacts the ability to represent such analogies in the embedding space. Do the visualizations suggest the embeddings learned by your model reflect meaningful semantic relationships?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6500906,
     "sourceId": 10499728,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
